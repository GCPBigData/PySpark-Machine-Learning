{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Dataset\n",
    "\n",
    "Here I quote a tutorial from [this blog](https://creativedata.atlassian.net/wiki/spaces/SAP/pages/83237142/Pyspark+-+Tutorial+based+on+Titanic+Dataset) using Pyspark mllib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import time\n",
    "import pyspark\n",
    "import os\n",
    "import csv\n",
    "from numpy import array\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Spark environment\n",
    "os.environ[\"HADOOP_USER_NAME\"] = \"hdfs\"\n",
    "os.environ[\"PYTHON_VERSION\"] = \"3.5.2\"\n",
    "conf = pyspark.SparkConf()\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mllib Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading from the hdfs, removing the header\n",
    "trainTitanic = sc.textFile(\"../data/titanic/train.csv\")\n",
    "trainHeader = trainTitanic.first()\n",
    "trainTitanic = trainTitanic.filter(lambda line: line != trainHeader).mapPartitions(lambda x: csv.reader(x))\n",
    "trainTitanic.first()\n",
    " \n",
    "# Data preprocessing\n",
    "def sexTransformMapper(elem):\n",
    "    '''Function which transform \"male\" into 1 and else things into 0\n",
    "    - elem : string\n",
    "    - return : vector\n",
    "    '''\n",
    "     \n",
    "    if elem == 'male' :\n",
    "        return [0]\n",
    "    else :\n",
    "        return [1]\n",
    "\n",
    "# Data Transformations and filter lines with empty strings\n",
    "trainTitanic=trainTitanic.map(lambda line: line[1:3]+sexTransformMapper(line[4])+line[5:11])\n",
    "trainTitanic=trainTitanic.filter(lambda line: line[3] != '' ).filter(lambda line: line[4] != '' )\n",
    "trainTitanic.take(10)\n",
    " \n",
    "# creating \"labeled point\" rdds specific to MLlib \"(label (v1, v2...vp])\"\n",
    "trainTitanicLP=trainTitanic.map(lambda line: LabeledPoint(line[0],[line[1:5]]))\n",
    "trainTitanicLP.first()\n",
    " \n",
    "# splitting dataset into train and test set\n",
    "(trainData, testData) = trainTitanicLP.randomSplit([0.7, 0.3])\n",
    " \n",
    "# Random forest : same parameters as sklearn (?)\n",
    "from pyspark.mllib.tree import RandomForest\n",
    " \n",
    "time_start=time.time()\n",
    "model_rf = RandomForest.trainClassifier(trainData, numClasses = 2,\n",
    "        categoricalFeaturesInfo = {}, numTrees = 100,\n",
    "        featureSubsetStrategy='auto', impurity='gini', maxDepth=12,\n",
    "        maxBins=32, seed=None)\n",
    " \n",
    "model_rf.numTrees()\n",
    "model_rf.totalNumNodes()\n",
    "time_end=time.time()\n",
    "time_rf=(time_end - time_start)\n",
    "print(\"RF takes %d s\" %(time_rf))\n",
    " \n",
    "# Predictions on test set\n",
    "predictions = model_rf.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    " \n",
    "# first metrics\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "metrics = BinaryClassificationMetrics(labelsAndPredictions)\n",
    " \n",
    "# Area under precision-recall curve\n",
    "print(\"Area under PR = %s\" % metrics.areaUnderPR)\n",
    " \n",
    "# Area under ROC curve\n",
    "print(\"Area under ROC = %s\" % metrics.areaUnderROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL Dataframe and Ml Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer, OneHotEncoder, VectorAssembler, IndexToString\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import *\n",
    " \n",
    "# Creatingt Spark SQL environment\n",
    "from pyspark.sql import SparkSession, HiveContext\n",
    "SparkContext.setSystemProperty(\"hive.metastore.uris\", \"thrift://nn1:9083\")\n",
    "spark = SparkSession.builder.enableHiveSupport().getOrCreate()\n",
    " \n",
    "# spark is an existing SparkSession\n",
    "train = spark.read.csv(\"../data/titanic/test.csv\", header = True)\n",
    "# Displays the content of the DataFrame to stdout\n",
    "train.show(10)\n",
    " \n",
    "# String to float on some columns of the dataset : creates a new dataset\n",
    "train = train.select(col(\"Survived\"),col(\"Sex\"),col(\"Embarked\"),col(\"Pclass\").cast(\"float\"),col(\"Age\").cast(\"float\"),col(\"SibSp\").cast(\"float\"),col(\"Fare\").cast(\"float\"))\n",
    " \n",
    "# dropping null values\n",
    "train = train.dropna()\n",
    " \n",
    "# Spliting in train and test set. Beware : It sorts the dataset\n",
    "(traindf, testdf) = train.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "train = StringIndexer(inputCol=\"Sex\", outputCol=\"indexedSex\").fit(train).transform(train)\n",
    "train = StringIndexer(inputCol=\"Embarked\", outputCol=\"indexedEmbarked\").fit(train).transform(train)\n",
    " \n",
    "train = StringIndexer(inputCol=\"Survived\", outputCol=\"indexedSurvived\").fit(train).transform(train)\n",
    " \n",
    "# One Hot Encoder on indexed features\n",
    "train = OneHotEncoder(inputCol=\"indexedSex\", outputCol=\"sexVec\").transform(train)\n",
    "train = OneHotEncoder(inputCol=\"indexedEmbarked\", outputCol=\"embarkedVec\").transform(train)\n",
    " \n",
    "# Feature assembler as a vector\n",
    "train = VectorAssembler(inputCols=[\"Pclass\",\"sexVec\",\"embarkedVec\", \"Age\",\"SibSp\",\"Fare\"],outputCol=\"features\").transform(train)\n",
    " \n",
    "rf = RandomForestClassifier(labelCol=\"indexedSurvived\", featuresCol=\"features\")\n",
    " \n",
    "model = rf.fit(train)\n",
    " \n",
    "predictions = model.transform(train)\n",
    " \n",
    "# Select example rows to display.\n",
    "predictions.select(col(\"prediction\"),col(\"probability\"),).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "genderIndexer = StringIndexer(inputCol=\"Sex\", outputCol=\"indexedSex\")\n",
    "embarkIndexer = StringIndexer(inputCol=\"Embarked\", outputCol=\"indexedEmbarked\")\n",
    " \n",
    "surviveIndexer = StringIndexer(inputCol=\"Survived\", outputCol=\"indexedSurvived\")\n",
    " \n",
    "# One Hot Encoder on indexed features\n",
    "genderEncoder = OneHotEncoder(inputCol=\"indexedSex\", outputCol=\"sexVec\")\n",
    "embarkEncoder = OneHotEncoder(inputCol=\"indexedEmbarked\", outputCol=\"embarkedVec\")\n",
    " \n",
    "# Create the vector structured data (label,features(vector))\n",
    "assembler = VectorAssembler(inputCols=[\"Pclass\",\"sexVec\",\"Age\",\"SibSp\",\"Fare\",\"embarkedVec\"],outputCol=\"features\")\n",
    " \n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"indexedSurvived\", featuresCol=\"features\")\n",
    " \n",
    "# Chain indexers and forest in a Pipeline\n",
    "pipeline = Pipeline(stages=[surviveIndexer, genderIndexer, embarkIndexer, genderEncoder,embarkEncoder, assembler, rf]) # genderIndexer,embarkIndexer,genderEncoder,embarkEncoder,\n",
    " \n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(traindf)\n",
    " \n",
    "# Predictions\n",
    "predictions = model.transform(testdf)\n",
    " \n",
    "# Select example rows to display.\n",
    "predictions.columns \n",
    " \n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"Survived\", \"features\").show(5)\n",
    " \n",
    "# Select (prediction, true label) and compute test error\n",
    "predictions = predictions.select(col(\"Survived\").cast(\"Float\"),col(\"prediction\"))\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Survived\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    " \n",
    "rfModel = model.stages[6]\n",
    "print(rfModel)  # summary only\n",
    " \n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Survived\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy = %g\" % accuracy)\n",
    " \n",
    "evaluatorf1 = MulticlassClassificationEvaluator(labelCol=\"Survived\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1 = evaluatorf1.evaluate(predictions)\n",
    "print(\"f1 = %g\" % f1)\n",
    " \n",
    "evaluatorwp = MulticlassClassificationEvaluator(labelCol=\"Survived\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "wp = evaluatorwp.evaluate(predictions)\n",
    "print(\"weightedPrecision = %g\" % wp)\n",
    " \n",
    "evaluatorwr = MulticlassClassificationEvaluator(labelCol=\"Survived\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "wr = evaluatorwr.evaluate(predictions)\n",
    "print(\"weightedRecall = %g\" % wr)\n",
    " \n",
    "# close sparkcontext\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
